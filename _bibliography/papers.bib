---
---

@string{aps = {American Physical Society,}}

@inproceedings{ntavelis2020aim,
  title={AIM 2020 challenge on image extreme inpainting},
  author={Ntavelis, Evangelos and Romero, Andr{\'e}s and Bigdeli, Siavash and Timofte, Radu and Hui, Zheng and Wang, Xiumei and Gao, Xinbo and Shin, Chajin and Kim, Taeoh and Son, Hanbin and Uddin, S M Nadim and others},
  booktitle={European Conference on Computer Vision},
  pages={716--741},
  year={2020},
  organization={Springer},
    url={https://openreview.net/forum?id=IDwN6xjHnK8},
  html={https://openreview.net/forum?id=IDwN6xjHnK8},
  selected={true},
  abbr={ICLR},
  tldr={A swin-transformer based auto-encoder structure is proposed that achieves SOTA in rate-distortion-complexity trade-off of image compression. It also outperforms SSF in video compression. AFAIK SwinT-ChARM is the first neural image codec that outperforms VTM in rate-distortion while with comparable decoding time on GPU.},
  image={/assets/img/10.jpg},
  abstract={Neural data compression based on nonlinear transform coding has made great progress over the last few years, mainly due to improvements in prior models, quantization methods and nonlinear transforms. A general trend in many recent works pushing the limit of rate-distortion performance is to use ever more expensive prior models that can lead to prohibitively slow decoding. Instead, we focus on more expressive transforms that result in a better rate-distortion-computation trade-off. Specifically, we show that nonlinear transforms built on Swin-transformers can achieve better compression efficiency than transforms built on convolutional neural networks (ConvNets), while requiring fewer parameters and shorter decoding time. Paired with a compute-efficient Channel-wise Auto-Regressive Model prior, our SwinT-ChARM model outperforms VTM-12.1 by 3.68 % in BD-rate on Kodak with comparable decoding speed. In P-frame video compression setting, we are able to outperform the popular ConvNet-based scale-space-flow model by 12.35 % in BD-rate on UVG. We provide model scaling studies to verify the computational efficiency of the proposed solutions and conduct several analyses to reveal the source of coding gain of transformers over ConvNets, including better spatial decorrelation, flexible effective receptive field, and more localized response of latent pixels during progressive decoding.}
}
@article{uddin2020global,
  title={Global and local attention-based free-form image inpainting},
  author={Uddin, S M Nadim and Jung, Yong Ju},
  journal={Sensors},
  volume={20},
  number={11},
  pages={3204},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}
@article{sharif2019edgenet,
  title={EdgeNet: A novel approach for Arabic numeral classification},
  author={Sharif, SMA and Mujtaba, Ghulam and Uddin, S M Nadim},
  journal={arXiv preprint arXiv:1908.02254},
  year={2019}
}
@inproceedings{uddin2016cognitive,
  title={Cognitive radio enabled vanet for multi-agent based intelligent traffic management system},
  author={Uddin, S M Nadim and Mansoor, Nafees and Hossain, Sazzad},
  booktitle={Proceedings of the First International Conference on Advanced Information and Communication Technology (ICAICT-16)},
  year={2016}
}
@inproceedings{uddin2016framework,
  title={A framework for event anomaly detection in cognitive radio based smart community},
  author={Uddin, S M Nadim and Mansoor, Nafees and Rahman, Musfiqur and Mohammed, Nabeel and Hossain, Sazzad},
  booktitle={2016 International Workshop on Computational Intelligence (IWCI)},
  pages={148--152},
  year={2016},
  organization={IEEE}
}
@inproceedings{ahmed2021deep,
  title={Deep Event Stereo Leveraged by Event-to-Image Translation},
  author={Ahmed, Soikat Hasan and Jang, Hae Woong and Uddin, S M Nadim and Jung, Yong Ju},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={2},
  pages={882--890},
  year={2021}
}