---
---

@string{aps = {American Physical Society,}}


@article{uddin2022unsupervised,
  title={Unsupervised Deep Event Stereo for Depth Estimation},
  author={Uddin, S M Nadim and Ahmed, Soikat Hasan and Jung, Yong Ju},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  volume={32},
  number={11},
  pages={7489--7504},
  year={2022},
  publisher={IEEE},
  url={https://ieeexplore.ieee.org/abstract/document/9819909},
  html={https://ieeexplore.ieee.org/abstract/document/9819909},
  abbr={Journal: TCSVT},
  selected={true},
  abstract={Bio-inspired event cameras have been considered effective alternatives to traditional frame-based cameras for stereo depth estimation, especially in challenging conditions such as low-light or high-speed environments. Recently, deep learning-based supervised event stereo matching methods have achieved significant performance improvements over the traditional event stereo methods. However, the supervised methods depend on ground-truth disparity maps for training, and it is difficult to secure a large amount of ground-truth disparity maps. A feasible alternative is to devise an unsupervised event stereo method that can be trained without ground-truth disparity maps. To this end, we propose the first unsupervised event stereo matching method that can predict dense disparity maps, and is trained by transforming the depth estimation problem into a warping-based reconstruction problem. We propose a novel unsupervised loss function that enforces the network to minimize the feature-level epipolar correlation difference between the ground-truth intensity images and warped images. Moreover, we propose a novel event embedding mechanism that utilizes both temporal and spatial neighboring events to capture spatio-temporal relationships among the events for stereo matching. Experimental results reveal that the proposed method outperforms the baseline unsupervised methods by significant margins (e.g., up to 16.88% improvement) and achieves comparable results with the existing supervised methods. Extensive ablation studies validate the efficacy of the proposed modules and architectural choices.}
}


@article{spencer2023second,
  title={The Second Monocular Depth Estimation Challenge},
  author={Spencer, Jaime and Qian, C Stella and Trescakova, Michaela and Russell, Chris and Hadfield, Simon and Graf, Erich W and Adams, Wendy J and Schofield, Andrew J and Elder, James and Bowden, Richard and Uddin, S M Nadim and others},
  journal={arXiv preprint arXiv:2304.07051},
  year={2023},
  selected={true},
  url={https://arxiv.org/abs/2304.07051},
  html={https://arxiv.org/abs/2304.07051},
  abbr={Conf: CVPRW},
  abstract={This paper discusses the results for the second edition of the Monocular Depth Estimation Challenge (MDEC). This edition was open to methods using any form of supervision, including fully-supervised, self-supervised, multi-task or proxy depth. The challenge was based around the SYNS-Patches dataset, which features a wide diversity of environments with high-quality dense ground-truth. This includes complex natural environments, e.g. forests or fields, which are greatly underrepresented in current benchmarks. The challenge received eight unique submissions that outperformed the provided SotA baseline on any of the pointcloud- or image-based metrics. The top supervised submission improved relative F-Score by 27.62%, while the top self-supervised improved it by 16.61%. Supervised submissions generally leveraged large collections of datasets to improve data diversity. Self-supervised submissions instead updated the network architecture and pretrained backbones. These results represent a significant progress in the field, while highlighting avenues for future research, such as reducing interpolation artifacts at depth boundaries, improving self-supervised indoor performance and overall natural image accuracy.}
}

@article{yoon2022multi,
  title={Multi-Scale Attention-Guided Non-Local Network for HDR Image Reconstruction},
  author={Yoon, Howoon and Uddin, S M Nadim and Jung, Yong Ju},
  journal={Sensors},
  selected={true},
  volume={22},
  number={18},
  pages={7044},
  year={2022},
  publisher={MDPI},
  abbr={Journal: Sensors},
  url={https://www.mdpi.com/1424-8220/22/18/7044},
  html={https://www.mdpi.com/1424-8220/22/18/7044},
  abstract={High-dynamic-range (HDR) image reconstruction methods are designed to fuse multiple Low-dynamic-range (LDR) images captured with different exposure values into a single HDR image. Recent CNN-based methods mostly perform local attention- or alignment-based fusion of multiple LDR images to create HDR contents. Depending on a single attention mechanism or alignment causes failure in compensating ghosting artifacts, which can arise in the synthesized HDR images due to the motion of objects or camera movement across different LDR image inputs. In this study, we propose a multi-scale attention-guided non-local network called MSANLnet for efficient HDR image reconstruction. To mitigate the ghosting artifacts, the proposed MSANLnet performs implicit alignment of LDR image features with multi-scale spatial attention modules and then reconstructs pixel intensity values using long-range dependencies through non-local means-based fusion. These modules adaptively select useful information that is not damaged by an object’s movement or unfavorable lighting conditions for image pixel fusion. Quantitative evaluations against several current state-of-the-art methods show that the proposed approach achieves higher performance than the existing methods. Moreover, comparative visual results show the effectiveness of the proposed method in restoring saturated information from original input images and mitigating ghosting artifacts caused by large movement of objects. Ablation studies show the effectiveness of the proposed method, architectural choices, and modules for efficient HDR reconstruction.}

}

@inproceedings{ntavelis2020aim,
  title={AIM 2020 challenge on image extreme inpainting},
  author={Ntavelis, Evangelos and Romero, Andr{\'e}s and Bigdeli, Siavash and Timofte, Radu and Hui, Zheng and Wang, Xiumei and Gao, Xinbo and Shin, Chajin and Kim, Taeoh and Son, Hanbin and Uddin, S M Nadim and others},
  booktitle={European Conference on Computer Vision},
  pages={716--741},
  selected={true},
  year={2020},
  organization={Springer},
  url={https://link.springer.com/chapter/10.1007/978-3-030-67070-2_43},
  html={https://link.springer.com/chapter/10.1007/978-3-030-67070-2_43},
  abbr={Journal: ECCV},
  abstract={This paper reviews the AIM 2020 challenge on extreme image inpainting. This report focuses on proposed solutions and results for two different tracks on extreme image inpainting: classical image inpainting and semantically guided image inpainting. The goal of track 1 is to inpaint large part of the image with no supervision. Similarly, the goal of track 2 is to inpaint the image by having access to the entire semantic segmentation map of the input. The challenge had 88 and 74 participants, respectively. 11 and 6 teams competed in the final phase of the challenge, respectively. This report gauges current solutions and set a benchmark for future extreme image inpainting methods.}
}
@article{uddin2020global,
  title={Global and local attention-based free-form image inpainting},
  author={Uddin, S M Nadim and Jung, Yong Ju},
  selected={true},
  journal={Sensors},
  volume={20},
  number={11},
  pages={3204},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute},
  url={https://www.mdpi.com/1424-8220/20/11/3204/htm},
  html={https://www.mdpi.com/1424-8220/20/11/3204/htm},
  abbr={Journal: Sensors},
  abstract={Deep-learning-based image inpainting methods have shown significant promise in both rectangular and irregular holes. However, the inpainting of irregular holes presents numerous challenges owing to uncertainties in their shapes and locations. When depending solely on convolutional neural network (CNN) or adversarial supervision, plausible inpainting results cannot be guaranteed because irregular holes need attention-based guidance for retrieving information for content generation. In this paper, we propose two new attention mechanisms, namely a mask pruning-based global attention module and a global and local attention module to obtain global dependency information and the local similarity information among the features for refined results. The proposed method is evaluated using state-of-the-art methods, and the experimental results show that our method outperforms the existing methods in both quantitative and qualitative measures}
}
@article{sharif2019edgenet,
  title={EdgeNet: A novel approach for Arabic numeral classification},
  author={Sharif, SMA and Mujtaba, Ghulam and Uddin, S M Nadim},
  journal={arXiv preprint arXiv:1908.02254},
  year={2019},
  url={https://arxiv.org/abs/1908.02254},
  html={https://arxiv.org/abs/1908.02254},
  abbr={Pre-print: ArXiv},
  abstract={Despite the importance of handwritten numeral classification, a robust and effective method for a widely used language like Arabic is still due. This study focuses to overcome two major limitations of existing works: data diversity and effective learning method. Hence, the existing Arabic numeral datasets have been merged into a single dataset and augmented to introduce data diversity. Moreover, a novel deep model has been proposed to exploit diverse data samples of unified dataset. The proposed deep model utilizes the low-level edge features by propagating them through residual connection. To make a fair comparison with the proposed model, the existing works have been studied under the unified dataset. The comparison experiments illustrate that the unified dataset accelerates the performance of the existing works. Moreover, the proposed model outperforms the existing state-of-the-art Arabic handwritten numeral classification methods and obtain an accuracy of 99.59% in the validation phase. Apart from that, different state-of-the-art classification models have studied with the same dataset to reveal their feasibility for the Arabic numeral classification.}
}
@inproceedings{uddin2016cognitive,
  title={Cognitive radio enabled vanet for multi-agent based intelligent traffic management system},
  author={Uddin, S M Nadim and Mansoor, Nafees and Hossain, Sazzad},
  booktitle={Proceedings of the First International Conference on Advanced Information and Communication Technology (ICAICT-16)},
  year={2016},
  abbr={Conf: ICAICT-16},
  url={https://www.researchgate.net/profile/Nafees-Mansoor/publication/317283710_A_Robust_Architecture_for_CR-VANET_in_Multi-agent_Based_Intelligent_Traffic_Management_System/links/59739324a6fdcc834882a803/A-Robust-Architecture-for-CR-VANET-in-Multi-agent-Based-Intelligent-Traffic-Management-System.pdf},
  html={https://www.researchgate.net/profile/Nafees-Mansoor/publication/317283710_A_Robust_Architecture_for_CR-VANET_in_Multi-agent_Based_Intelligent_Traffic_Management_System/links/59739324a6fdcc834882a803/A-Robust-Architecture-for-CR-VANET-in-Multi-agent-Based-Intelligent-Traffic-Management-System.pdf},
  abstract={With the mounting interest on cognitive radio (CR) technology in wireless communication systems, it is anticipated that CR-enabled vehicular networks will play a vigorous role in the enrichment of communication efficiency in vehicular network. This paper presents a Cognitive Radio enabled VANET for multi-agent based intelligent traffic management system. A skeleton for intelligent learning and decision mechanism for Central Traffic Management is also proposed and discussed in the model. The proposed model has two distinct information exchange system layouts. One is dynamic (vehicle to vehicle) and another is semi-dynamic (vehicle to Road-Side-Unit). For the vehicle-2-vehicle communication, the proposed model assumes that vehicles can communicate with each other using available wireless resources with the help cognitive radio mechanism. This paper also introduces a cluster formation scheme for better accuracy in data transmission among vehicles. The dynamic module of the proposed model is later simulated and validated for some important performance communication metrics.}
}
@inproceedings{uddin2016framework,
  title={A framework for event anomaly detection in cognitive radio based smart community},
  author={Uddin, S M Nadim and Mansoor, Nafees and Rahman, Musfiqur and Mohammed, Nabeel and Hossain, Sazzad},
  booktitle={2016 International Workshop on Computational Intelligence (IWCI)},
  pages={148--152},
  year={2016},
  url={https://ieeexplore.ieee.org/abstract/document/7860356},
  html={https://ieeexplore.ieee.org/abstract/document/7860356},
  abbr={Conf: IEEE IWCI},
  organization={IEEE},
  abstract={With the advancement of technology, a surge of research interest in cognitive radio based networks in smart communities has been mounting. It is anticipated that CR-enabled networks will play a vigorous role in the enrichment of communication efficiency in neighborhood sensor area network. This paper presents a framework for Cognitive Radio based event anomaly detection mechanism. A skeleton for intelligent learning, detection and decision mechanism for Local Controller Unit and a Primary Controller Unit is also proposed and discussed in the model. The proposed model has four distinct layers namely sensors, routers, Local Controller Unit and Primary Controller Unit. A scheme for emergency situation detection and notification has been proposed. This paper also introduces a cluster formation scheme for better accuracy in data transmission among different hierarchical layers. The network module of the proposed model is later simulated and validated for some important performance communication metrics.}
}
@inproceedings{ahmed2021deep,
  title={Deep Event Stereo Leveraged by Event-to-Image Translation},
  author={Ahmed, Soikat Hasan and Jang, Hae Woong and Uddin, S M Nadim and Jung, Yong Ju},
  selected={true},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={2},
  pages={882--890},
  year={2021},
  abbr={Conf: AAAI-21},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/16171},
  html={https://ojs.aaai.org/index.php/AAAI/article/view/16171},
  abstract = {Depth estimation in real-world applications requires precise responses to fast motion and challenging lighting conditions. Event cameras use bio-inspired event-driven sensors that provide instantaneous and asynchronous information of pixel-level log intensity changes, which makes them suitable for depth estimation in such challenging conditions. However, as the event cameras primarily provide asynchronous and spatially sparse event data, it is hard to provide accurate dense disparity map in stereo event camera setups - especially in estimating disparities on local structures or edges. In this study, we develop a novel deep event stereo network that reconstructs spatial intensity image features from embedded event streams and leverages the event features using the reconstructed image features to compute dense disparity maps. To this end, we propose a novel event-to-image translation network with a cross-semantic attention mechanism that calculates the global semantic context of the event features for the intensity image reconstruction. In addition, a feature aggregation module is developed for accurate disparity estimation, which modulates the event features with the reconstructed image features by a stacked dilated spatially-adaptive denormalization mechanism. Experimental results reveal that our method can outperform the state-of-the-art methods by significant margins both in quantitative and qualitative measures.}

}
@article{uddin2022sifnet,
  title={SIFNet: Free-form image inpainting using color split-inpaint-fuse approach},
  author={Uddin, S M Nadim and Jung, Yong Ju},
  selected={true},
  journal={Computer Vision and Image Understanding},
  volume={221},
  pages={103446},
  year={2022},
  publisher={Elsevier},
  url={https://www.sciencedirect.com/science/article/pii/S1077314222000613?dgcid=coauthor},
  html={https://www.sciencedirect.com/science/article/pii/S1077314222000613?dgcid=coauthor},
  abbr={Journal: CVIU},
  abstract={Recent deep learning-based approaches have shown outstanding performance in generating visually plausible and refined contents for the missing regions in free-form image inpainting tasks. However, most of the existing methods employ a coarse-to-refine approach where the refinement process depends on a single coarse estimation, often leading to texture and structure inconsistencies. Though several existing methods focus on incorporating additional inputs to mitigate this problem, no learning-based studies have investigated the effects of decomposing input corrupted image into luma and chroma images and performing decoupled inpainting of the decomposed components. To this end, we propose a Split-Inpaint-Fuse Network (SIFNet), an end-to-end two-stage inpainting approach that uses a split-inpaint sub-network for separately inpainting the corrupted luma and chroma images using two decoupled branches in the coarse stage and a fusion sub-network for fusing the inpainted luma and chroma images into a refined image in the refinement stage. Additionally, we propose two attention mechanisms for the coarse stage – a progressive context module to find the patch-level feature similarity for the luma image reconstruction and a spatial-channel context module to find important spatial and channel features for the chroma image reconstruction. Experimental results reveal that our Split-Inpaint-Fuse approach outperforms the existing inpainting methods by comparative margins. In addition, extensive ablation studies confirm the effectiveness of the proposed approach, constituting modules and architectural choices.}
}